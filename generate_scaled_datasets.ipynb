{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate_datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, \\\n",
    "  RobustScaler, QuantileTransformer, PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_DIR = \"datasets\"\n",
    "OUT_DIR = \"scaled_datasets\"\n",
    "RANDOM_STATE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OUT_DIR):\n",
    "  os.mkdir(OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "ratios, weights = generate_datasets.get_imbalance()\n",
    "for dataset in os.listdir(DATASETS_DIR):\n",
    "  dataset_path = os.path.join(DATASETS_DIR, dataset)\n",
    "  dataset_names = [dataset + f'_w_{w:.3f}.csv' for w in weights]\n",
    "  datasets_df = [pd.read_csv(os.path.join(dataset_path, dataset_name)) for dataset_name in dataset_names]\n",
    "  datasets.append(datasets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {\n",
    "  'MM': MinMaxScaler(),\n",
    "  'SS': StandardScaler(),\n",
    "  'MA': MaxAbsScaler(),\n",
    "  'RS': RobustScaler(),\n",
    "  'QT': QuantileTransformer(),\n",
    "  'PT': PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for name, scaler in scalers.items():\n",
    "  scaled_datasets = []\n",
    "  for i in range(len(datasets)):\n",
    "    scaled_weight = []\n",
    "    for j in range(len(weights)):\n",
    "      dataset = datasets[i][j]\n",
    "      X = dataset.iloc[:, :-1]\n",
    "      y = dataset.iloc[:, -1]\n",
    "      X = scaler.fit_transform(X)\n",
    "      scaled_dataset = pd.concat([pd.DataFrame(X), y], axis=1)\n",
    "      scaled_weight.append(scaled_dataset)\n",
    "    scaled_datasets.append(scaled_weight)\n",
    "  results[name] = scaled_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(datasets)):\n",
    "  for j in range(len(weights)):\n",
    "    base_path = f\"{OUT_DIR}/dataset_{i+1}/w_{weights[j]:.3f}\"\n",
    "    if not os.path.exists(base_path):\n",
    "      os.makedirs(base_path)\n",
    "    datasets[i][j].to_csv(f\"{base_path}/original.csv\", index=False)\n",
    "    for name, result in results.items():\n",
    "      results[name][i][j].to_csv(f\"{base_path}/{name}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
