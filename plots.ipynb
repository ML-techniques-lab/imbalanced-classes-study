{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PATH = \"./run.db\"\n",
    "PLOT_PATH = './plots/results'\n",
    "METRICS = ['accuracy', 'f1_score', 'g_mean', 'roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PLOT_PATH):\n",
    "  os.makedirs(PLOT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_without_dataset():\n",
    "  sqlite_connection = sqlite3.connect(DB_PATH)\n",
    "  query = \"\"\"\n",
    "    select\n",
    "      model,\n",
    "      scaling,\n",
    "      level,\n",
    "      avg(accuracy),\n",
    "      avg(f1_score),\n",
    "      avg(g_mean),\n",
    "      avg(roc_auc)\n",
    "    from result\n",
    "    group by\n",
    "      model,\n",
    "      scaling,\n",
    "      level;\n",
    "    \"\"\"\n",
    "  cursor = sqlite_connection.cursor()\n",
    "  cursor.execute(query)\n",
    "  data = cursor.fetchall()\n",
    "  result_df = pd.DataFrame(data, columns=['model', 'scaling', 'level', 'accuracy', 'f1_score', 'g_mean', 'roc_auc'])\n",
    "  result_df['level'] = result_df.level.apply(lambda x: int(x.split()[1]))\n",
    "  return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_data():\n",
    "  sqlite_connection = sqlite3.connect(DB_PATH)\n",
    "  query = \"\"\"\n",
    "    select\n",
    "      model,\n",
    "      scaling,\n",
    "      level,\n",
    "      dataset,\n",
    "      avg(accuracy),\n",
    "      avg(f1_score),\n",
    "      avg(g_mean),\n",
    "      avg(roc_auc)\n",
    "    from result\n",
    "    group by\n",
    "      model,\n",
    "      scaling,\n",
    "      level,\n",
    "      dataset;\n",
    "    \"\"\"\n",
    "  cursor = sqlite_connection.cursor()\n",
    "  cursor.execute(query)\n",
    "  data = cursor.fetchall()\n",
    "  result_df = pd.DataFrame(data, columns=['model', 'scaling', 'level', 'dataset', 'accuracy', 'f1_score', 'g_mean', 'roc_auc'])\n",
    "  result_df['level'] = result_df.level.apply(lambda x: int(x.split()[1]))\n",
    "  result_df['dataset'] = result_df.dataset.apply(lambda x: int(x.split()[1]))\n",
    "  return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_by_level():\n",
    "  # Gera gráfico de linha de performance por nível\n",
    "  result_df = fetch_data_without_dataset()\n",
    "  models = result_df['model'].unique()\n",
    "  scalings = result_df['scaling'].unique()\n",
    "  div = len(models) // 3\n",
    "  for i in range(3):\n",
    "    curr_models = models[i*div : (i+1)*div]\n",
    "    plot_data = result_df.query(f'model.isin(@curr_models)')\n",
    "    grid = sns.FacetGrid(plot_data.melt(id_vars=['model', 'scaling', 'level']), row='scaling', col='model', margin_titles=True, sharex=False, sharey=False)\n",
    "    grid.map_dataframe(sns.lineplot, 'level', 'value', hue='variable')\n",
    "    grid.add_legend(fontsize=15)\n",
    "    grid.set_titles(col_template='{col_name}', row_template=\"\", size=15)\n",
    "    grid.tick_params(axis='y', labelsize=12)\n",
    "    grid.set_xlabels(size=12)\n",
    "    for j in range(grid.axes.shape[0]):\n",
    "      grid.axes[j,0].set_ylabel(f\"{scalings[j]}\", size=15)\n",
    "    grid.tight_layout()\n",
    "    grid.savefig(f\"{PLOT_PATH}/level_by_performance_{i+1}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_average_diff_table(metric, abs=True):\n",
    "  # Gera tabela com a diferença média entre cada normalização e o modelo original, dado uma métrica\n",
    "  result_df = fetch_all_data()\n",
    "  models = result_df['model'].unique()\n",
    "  scalings = result_df['scaling'].unique()\n",
    "  plot_diffs = {}\n",
    "  for model in models:\n",
    "    diffs = []\n",
    "    original_scale_df = result_df.query(f\"model == '{model}' and scaling == 'original'\")\n",
    "    methods = [scaling for scaling in scalings if scaling != 'original']\n",
    "    for scaling in methods:\n",
    "      plot_data = result_df.query(f\"model == '{model}' and scaling == '{scaling}'\")\n",
    "      diff = plot_data[metric].to_numpy() - original_scale_df[metric].to_numpy()\n",
    "      if abs:\n",
    "        diff = np.abs(diff)\n",
    "      diffs.append(np.average(diff))\n",
    "    plot_diffs[model] = diffs\n",
    "  return pd.DataFrame.from_records(plot_diffs, index=methods).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_diff_by_dataset():\n",
    "  # Gera gráfico de diferença média de cada dataset para cada modelo\n",
    "  # Gera um gráfico para cada métrica\n",
    "  result_df = fetch_all_data()\n",
    "  models = result_df['model'].unique()\n",
    "  scalings = result_df['scaling'].unique()\n",
    "  methods = [method for method in scalings if method != 'original']\n",
    "  div = len(models) // 3\n",
    "  for metric in METRICS:\n",
    "    for d in range(3):\n",
    "      fig, axes = plt.subplots(5, 7, figsize=(30, 15))\n",
    "      for i in range(7):\n",
    "        model = models[(d*div)+i]\n",
    "        for j in range(len(methods)):\n",
    "          method = methods[j]\n",
    "          diffs = []\n",
    "          for dataset in range(1, 101):\n",
    "            model_df = result_df.query(f\"model == '{model}' and dataset == {dataset} and scaling == '{method}'\")\n",
    "            original_scale_df = result_df.query(f\"model == '{model}' and dataset == {dataset} and scaling == 'original'\")\n",
    "            diff = model_df.query(f\"scaling == '{method}'\")[metric].to_numpy() - original_scale_df[metric].to_numpy()\n",
    "            diffs.append(np.mean(diff))\n",
    "          ax = axes[j][i]\n",
    "          sns.barplot(diffs, ax=ax)\n",
    "          ax.set_ylabel(\"\")\n",
    "          ax.set_xlabel(\"\")\n",
    "          ax.set_xticks([])\n",
    "          ax.tick_params(axis='y', labelsize=12)\n",
    "          ax.set(ylim=(-1,1))\n",
    "          if j == 0:\n",
    "            ax.set_title(model, size=15)\n",
    "          if i == 0:\n",
    "            ax.set_ylabel(method, size=15)\n",
    "      plt.tight_layout()\n",
    "      plt.savefig(f\"{PLOT_PATH}/datasets_{metric}_{d+1}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_range():\n",
    "  # Gera gráfico da diferença entre a maior e a menor performance por nível, para cada modelo\n",
    "  result_df = fetch_data_without_dataset()\n",
    "  models = result_df['model'].unique()\n",
    "  plot_results = []\n",
    "  for i in range(len(models)):\n",
    "    model = models[i]\n",
    "    for level in range(1, 11):\n",
    "      model_level = result_df.query(f\"model == '{model}' and level == {level}\")\n",
    "      accuracy = model_level['accuracy']\n",
    "      f1_score = model_level['f1_score']\n",
    "      g_mean = model_level['g_mean']\n",
    "      roc_auc = model_level['roc_auc']\n",
    "      accuracy_diff = accuracy.max() - accuracy.min()\n",
    "      f1_score_diff = f1_score.max() - f1_score.min()\n",
    "      g_mean_diff = g_mean.max() - g_mean.min()\n",
    "      roc_auc_diff = roc_auc.max() - roc_auc.min()\n",
    "      plot_results.append((level, accuracy_diff, f1_score_diff, g_mean_diff, roc_auc_diff, model))\n",
    "  plot_df = pd.DataFrame(plot_results, columns=['level', 'accuracy', 'f1_score', 'g_mean', 'roc_auc', 'model'])\n",
    "  grid = sns.catplot(plot_df.melt(id_vars=['level', 'model']), kind='bar', col='model', x='level', y='value', hue='variable', col_wrap=3, sharex=False, sharey=False)\n",
    "  grid.set_titles(col_template='{col_name}', row_template='', size=15)\n",
    "  grid.set(ylim=(0, 1.1))\n",
    "  grid.set_xlabels(size=12)\n",
    "  grid.tick_params(axis='both', labelsize=12)\n",
    "  sns.move_legend(grid, \"center right\", fontsize=15, bbox_to_anchor=(1.05, 0.5))\n",
    "  # grid.add_legend(fontsize=15)\n",
    "  for ax in grid.axes.flat:\n",
    "    ax.set_ylabel(\"\")\n",
    "  grid.tight_layout()\n",
    "  grid.savefig(f\"{PLOT_PATH}/range.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_range_for_each_dataset():\n",
    "  # Gera o plot de range para cada dataset individual e para cada modelo\n",
    "  result_df = fetch_all_data()\n",
    "  models = result_df['model'].unique()\n",
    "  for model in models:\n",
    "    plot_results = []\n",
    "    for i in range(1, 101):\n",
    "      for level in range(1, 11):\n",
    "        dataset_df = result_df.query(f\"model == '{model}' and level == {level} and dataset == {i}\")\n",
    "        accuracy = dataset_df['accuracy']\n",
    "        f1_score = dataset_df['f1_score']\n",
    "        g_mean = dataset_df['g_mean']\n",
    "        roc_auc = dataset_df['roc_auc']\n",
    "        accuracy_diff = accuracy.max() - accuracy.min()\n",
    "        f1_score_diff = f1_score.max() - f1_score.min()\n",
    "        g_mean_diff = g_mean.max() - g_mean.min()\n",
    "        roc_auc_diff = roc_auc.max() - roc_auc.min()\n",
    "        plot_results.append((level, accuracy_diff, f1_score_diff, g_mean_diff, roc_auc_diff, i))\n",
    "    plot_df = pd.DataFrame(plot_results, columns=['level', 'accuracy', 'f1_score', 'g_mean', 'roc_auc', 'dataset'])\n",
    "    grid = sns.catplot(plot_df.melt(id_vars=['level', 'dataset']), kind='bar', col='dataset', x='level', y='value', hue='variable', sharex=False, sharey=False, col_wrap=10, width=0.9)\n",
    "    grid.set_titles(col_template='dataset {col_name}', row_template='', size=15)\n",
    "    grid.set(ylim=(0, 1.1))\n",
    "    grid.tick_params(axis='both', labelsize=12)\n",
    "    sns.move_legend(grid, \"center right\", fontsize=20, title=None, bbox_to_anchor=(1.05, 0.5))\n",
    "    for ax in grid.axes.flat:\n",
    "      ax.set_xlabel(\"\")\n",
    "      ax.set_ylabel(\"\")\n",
    "    grid.tight_layout()\n",
    "    grid.savefig(f\"{PLOT_PATH}/range_dataset/range_datasets_{model}.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
